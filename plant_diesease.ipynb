{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plant diesease.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "toc_visible": true,
      "mount_file_id": "1SVbbvAzOuIJLmPQq7cTRxz4uFYS6f3Rq",
      "authorship_tag": "ABX9TyNMEJZoUyXVqKPCnU6yCpfS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NKASG/deep-learning-keras-tf-tutorial/blob/master/plant_diesease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUnv17fCuRnP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import models, layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = image_dataset_from_directory('/content/drive/MyDrive/Download',shuffle=True, image_size=(256,256), batch_size=32 )"
      ],
      "metadata": {
        "id": "-UfgEZgCuej3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset.class_names\n",
        "class_names"
      ],
      "metadata": {
        "id": "XZBuBDYzXGRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "RNZiVeapX294"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, labels_batch in dataset.take(1):\n",
        "    print(image_batch.shape)\n",
        "    print(labels_batch.numpy())\n",
        "    "
      ],
      "metadata": {
        "id": "uWvtpTIRYECQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of our data set"
      ],
      "metadata": {
        "id": "A0WkzOuMdgRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for image_batch, labels_batch in dataset.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(image_batch[i].numpy().astype('uint8'))\n",
        "        plt.title(class_names[labels_batch[i]])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "R0xtNUPHcTk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i want to split the date into 3 categories which are training_data,validation_data,Test_data which are 80%, 10%, 10% respectively which by calculation with length of 682 the training_data = 546 , validation_data = 68 and Test_data = 68"
      ],
      "metadata": {
        "id": "b2Q502R5doWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_len = len(dataset)\n",
        "total_len"
      ],
      "metadata": {
        "id": "9Vxf1PfIcxBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_no = total_len * 0.8\n",
        "train_no"
      ],
      "metadata": {
        "id": "NfCLrgTof_83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_no = round(train_no)"
      ],
      "metadata": {
        "id": "8loKhZKBn5tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset.take(train_no)\n",
        "len(train_ds)"
      ],
      "metadata": {
        "id": "NKiJVoPggysf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remain = dataset.skip(train_no)\n",
        "len(remain)"
      ],
      "metadata": {
        "id": "0dDrshTTiBFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_no = total_len * 0.1\n",
        "val_no"
      ],
      "metadata": {
        "id": "i-R9MVupj1bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_no = round(val_no)"
      ],
      "metadata": {
        "id": "MBx21j1zoph5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = remain.take(val_no)\n",
        "len(val_ds)"
      ],
      "metadata": {
        "id": "wFSyftYOkXWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = remain.skip(val_no)\n",
        "len(test_ds)"
      ],
      "metadata": {
        "id": "np-y1IqqlGI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prefetch and cache is used when loadingg batches while GPU is loading a batch the GPU will be loading the next batch its help saving time when reading those images while shuffle is needed for the image to be choosen randomly"
      ],
      "metadata": {
        "id": "yFjgCTVGmSKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "8QJwK8SkmQVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing of data by dividing each images by 255 and uniform resizing of the image to 255x255"
      ],
      "metadata": {
        "id": "MP_avTVBo_yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.Resizing(256,256),\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "])"
      ],
      "metadata": {
        "id": "HQbi-1ISpVrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation this is needed when we want to address over fitting and to boost accuracy"
      ],
      "metadata": {
        "id": "cPabKEWHq1C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "d77y_oFarLa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (32,256,256,3)"
      ],
      "metadata": {
        "id": "NqNr0W1iKSOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    Model = keras.models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    data_augmentation,\n",
        "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    \n",
        "    layers.Dense(3, activation='softmax'),\n",
        "    \n",
        "])\n",
        "     \n",
        "   "
      ],
      "metadata": {
        "id": "LQM-9EFr19A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Model.build(input_shape)"
      ],
      "metadata": {
        "id": "s0HdBbAErMjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "C5ce67BEtmE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model.summary()"
      ],
      "metadata": {
        "id": "Bl3Aq4qxAMeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = Model.fit( train_ds, batch_size=32, validation_data=val_ds, verbose=1, epochs=10)"
      ],
      "metadata": {
        "id": "pNCs4d6BDRuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = Model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "d2oXAG2EwVzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "nogKdVEcUI9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy', )\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy','val_acc'], loc= 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TawNlkPcUWZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss','val_loss'], loc= 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g8Ipv5q5UiOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model prediction "
      ],
      "metadata": {
        "id": "Si1KHouVwfkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "for images_batch, labels_batch in test_ds.take(1):\n",
        "    \n",
        "    first_image = images_batch[0].numpy().astype('uint8')\n",
        "    first_label = labels_batch[0].numpy()\n",
        "    \n",
        "    print(\"first image to predict\")\n",
        "    plt.imshow(first_image)\n",
        "    print(\"actual label:\",class_names[first_label])\n",
        "    \n",
        "    batch_prediction = Model.predict(images_batch)\n",
        "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
      ],
      "metadata": {
        "id": "v72pbf8qySci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    predicted_class = class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
        "    return predicted_class, confidence"
      ],
      "metadata": {
        "id": "l05uEqomv-7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        \n",
        "        predicted_class, confidence = predict(Model, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]] \n",
        "        \n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
        "        \n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "4WE_Bv2yveV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Gradio"
      ],
      "metadata": {
        "id": "LjiEdDtkk2SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gradio\n"
      ],
      "metadata": {
        "id": "WdkqWKmGkzQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import gradio as gr"
      ],
      "metadata": {
        "id": "DLtXCreypB6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def predict_image(img):\n",
        "  #img_4d=img.reshape(-1,256,256,3)\n",
        " # prediction=Model.predict(img_4d)[0]\n",
        "#  return {class_names[i]: float(prediction[i]) for i in range(3)}"
      ],
      "metadata": {
        "id": "LOGolz2boTj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image = gr.inputs.Image(shape=(256,256))\n",
        "#label = gr.outputs.Label(num_top_classes=3)\n",
        "\n",
        "#gr.Interface(fn=predict_image, inputs=image, outputs=label,interpretation='default').launch()"
      ],
      "metadata": {
        "id": "LjM_UC4_o9ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model.save(\"/content/sample_data\")"
      ],
      "metadata": {
        "id": "xrDFB1EIE1Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "0E6T7JI11kBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/qkeras.git \n",
        "import sys \n",
        "sys.path.append('qkeras') \n",
        "!pip install git+https://github.com/keras-team/keras-tuner.git \n",
        "!pip install tensorflow_model_optimization \n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "pNIYzTCv0t6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --user --upgrade tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "wGKcTOUWRnAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To install dependencies on Ubuntu:\n",
        "# sudo apt-get install bazel git python-pip\n",
        "# For other platforms, see Bazel docs above.\n",
        "git clone https://github.com/tensorflow/model-optimization.git\n",
        "cd model-optimization\n",
        "bazel build --copt=-O3 --copt=-march=native :pip_pkg\n",
        "PKGDIR=$(mktemp -d)\n",
        "./bazel-bin/pip_pkg $PKGDIR\n",
        "pip install --user --upgrade $PKGDIR/*.whl"
      ],
      "metadata": {
        "id": "iVoVl7iv3iu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(Model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "id": "_5wCZ3FEQ5OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_version=max([int(i) for i in os.listdir(\"../models\") + [0]])+1\n",
        "model.save(f\"../models/{model_version}\")"
      ],
      "metadata": {
        "id": "yT6wPk4H7FOw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}